{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      0.51      0.68      3110\n",
      "\n",
      "    accuracy                           0.51      3111\n",
      "   macro avg       0.50      0.25      0.34      3111\n",
      "weighted avg       1.00      0.51      0.68      3111\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0    1]\n",
      " [1524 1586]]\n",
      "F1 Score: 0.6751075989263068\n",
      "Accuracy: 0.5098039215686274\n",
      "Elapsed Time: 0.16756105422973633 seconds\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "import pandas as pd\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import time\n",
    "\n",
    "# Import the Punkt tokenizer from the nltk.data module\n",
    "nltk.data.path.append(\"nltk_data\")  # Specify the path to the nltk_data directory if necessary\n",
    "custom_sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Read the contents of the file\n",
    "file_path = \"europarl_en.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Remove newline characters and tab characters\n",
    "text = text.replace('\\\\r', '').replace('\\\\n', '').replace('\\\\t', '')\n",
    "\n",
    "# Define custom sentence boundaries\n",
    "custom_sent_tokenize._params.abbrev_types.update(['.', ',', ';', 'e.g.', 'i.e.', 'etc.', 'mr.', 'mrs.', 'ms.', 'dr.'])\n",
    "custom_sent_tokenize._params.abbrev_types.update(['v.', 'vs.', 'art.', 'no.', 'jr.', 'sr.', 'inc.', 'co.'])\n",
    "\n",
    "# Measure the time taken for sentence boundary detection\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "predicted_sentences = custom_sent_tokenize.tokenize(text)\n",
    "\n",
    "# Measure the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Read the labeled dataset for evaluation\n",
    "labeled_dataset_path = 'sentences_labels.csv'\n",
    "df = pd.read_csv(labeled_dataset_path)\n",
    "\n",
    "# Get the expected sentences and labels from the labeled dataset\n",
    "expected_sentences = df['Sentence'].tolist()\n",
    "expected_labels = df['Label'].tolist()\n",
    "\n",
    "# Filter out sentences from the labeled dataset that are not present in the predicted sentences\n",
    "filtered_expected_labels = []\n",
    "filtered_expected_sentences = []\n",
    "for sent, label in zip(expected_sentences, expected_labels):\n",
    "    if sent in predicted_sentences:\n",
    "        filtered_expected_sentences.append(sent)\n",
    "        filtered_expected_labels.append(label)\n",
    "\n",
    "# Create a list to store the predicted labels\n",
    "predicted_labels = []\n",
    "for sent in filtered_expected_sentences:\n",
    "    idx = predicted_sentences.index(sent)\n",
    "    predicted_labels.append(expected_labels[idx])\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for true_label, pred_label in zip(filtered_expected_labels, predicted_labels) if true_label == pred_label)\n",
    "total_predictions = len(filtered_expected_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(filtered_expected_labels, predicted_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(filtered_expected_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(filtered_expected_labels, predicted_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Print accuracy and elapsed time\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mr. Johnson, the CEO of XYZ Corp., arrived at the meeting at 9:00 a.m. with his wife, Mrs. Johnson!\n",
      "2 They were greeted by the board members, including Dr. Anderson, the company's Chief Financial Officer, and Ms. Thompson, the Head of Marketing?\n",
      "3 The meeting room was adorned with elegant decorations and state-of-the-art technology!\n",
      "4 During the meeting, they discussed various topics, including financial strategies, marketing campaigns, and employee engagement.\n",
      "5 They analyzed the current market trends, reviewed sales reports, and brainstormed innovative ideas.\n",
      "6 It was a high-energy session with everyone actively participating and sharing their perspectives!\n",
      "7 The meeting lasted for hours, and they covered a wide range of issues.\n",
      "8 They deliberated on cost-cutting measures, expansion plans, and investment opportunities.\n",
      "9 They also addressed concerns raised by the staff, such as work-life balance and career development!\n",
      "10 The atmosphere was intense yet collaborative, with each person expressing their opinions respectfully.\n",
      "11 In addition to the main agenda, they decided to implement new policies to improve customer satisfaction, i.e., enhancing the quality of products and providing better after-sales service.\n",
      "12 They planned to conduct customer surveys, gather feedback, and tailor their offerings based on customer preferences.\n",
      "13 The focus was on delivering exceptional customer experiences and building long-term relationships!\n",
      "14 Furthermore, they discussed their marketing strategy, which included targeted advertising campaigns, social media promotions, and collaborations with influencers!\n",
      "15 They aimed to reach a wider audience and increase brand visibility.\n",
      "16 They also planned to launch a new product line targeting young consumers, e.g., Gen Z and millennials.\n",
      "17 Extensive market research and product development were in progress to meet the specific needs and preferences of these demographics!\n",
      "18 In conclusion, the meeting was productive, and the decisions made will have a significant impact on the company's future success!\n",
      "19 The collective efforts of the team, led by Mr. Johnson and supported by the dedicated board members, would propel the company to new heights!\n",
      "20 The commitment to excellence and constant innovation would position XYZ Corp. as a market leader and drive sustainable growth.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# Import the Punkt tokenizer from the nltk.data module\n",
    "nltk.data.path.append(\"nltk_data\")\n",
    "custom_sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define your custom sentence\n",
    "#custom_sentence = \"This is a custom sentence. It has multiple sentences. Can the model detect the boundaries?\"\n",
    "\n",
    "with open('sample.txt', 'r') as file:\n",
    "    custom_sentence = file.read()\n",
    "\n",
    "# Tokenize the custom sentence into sentences\n",
    "predicted_sentences = custom_sent_tokenize.tokenize(custom_sentence)\n",
    "\n",
    "# Print the separated sentences\n",
    "i=0\n",
    "for sentence in predicted_sentences:\n",
    "    i=i+1\n",
    "    print(i, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 1.0521612167358398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3244\n",
      "           1       1.00      1.00      1.00      3244\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      6488\n",
      "   macro avg       1.00      1.00      1.00      6488\n",
      "weighted avg       1.00      1.00      1.00      6488\n",
      " samples avg       1.00      1.00      1.00      6488\n",
      "\n",
      "[[[   7    0]\n",
      "  [   0 3244]]\n",
      "\n",
      " [[   0    7]\n",
      "  [   0 3244]]]\n",
      "Accuracy: 0.9978468163641956\n",
      "F1 score: 0.9994611239414934\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "\n",
    "\n",
    "# Defining Feature and Label Functions\n",
    "def extract_features(sentence, i):\n",
    "    features = {\n",
    "        'word': sentence[i],\n",
    "        'is_first': i == 0,\n",
    "        'is_last': i == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[i][0].upper() == sentence[i][0],\n",
    "        'is_all_caps': sentence[i].upper() == sentence[i],\n",
    "        'is_all_lower': sentence[i].lower() == sentence[i],\n",
    "        'is_alphanumeric': int(bool((len(sentence[i]) > 1) & sentence[i].isalnum()))\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_label(sentence, i):\n",
    "    last_char = sentence[i][-1]\n",
    "    return str(last_char == ',' or last_char == ';' or last_char == ':' or last_char == '.' or last_char == '?')\n",
    "\n",
    "\n",
    "# read text from file\n",
    "with open('europarl_en.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.replace('\\\\r', '').replace('\\\\n', '').replace('\\\\t', '')\n",
    "\n",
    "# perform sentence boundary detection\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "X = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "y = [[get_label(sentence, i) for i in range(len(sentence))] for sentence in X]\n",
    "\n",
    "# extract features from sentences\n",
    "X_feats = [[extract_features(X[i], j) for j in range(len(X[i]))] for i in range(len(X))]\n",
    "\n",
    "# Convert labels to list of sequences\n",
    "y_seq = [sentence_labels for sentence_labels in y]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# train CRF model\n",
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=3)\n",
    "crf.fit(X_feats, y_seq)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time\",elapsed_time)\n",
    "\n",
    "# test CRF model on training data\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_bin = mlb.fit_transform(y_seq)\n",
    "\n",
    "# test CRF model on training data\n",
    "y_pred = crf.predict(X_feats)\n",
    "\n",
    "# Convert y_pred to binary array format (if needed)\n",
    "y_pred_bin = mlb.transform(y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_bin, y_pred_bin)\n",
    "print(report)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_matrix = multilabel_confusion_matrix(y_bin, y_pred_bin)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_bin, y_pred_bin)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = metrics.f1_score(y_bin, y_pred_bin, average='weighted')\n",
    "print('F1 score:', f1_score)\n",
    "\n",
    "# Print boundary-detected text\n",
    "for sentence, labels in zip(X, y_pred):\n",
    "    boundary_detected_text = \" \".join([token for token, label in zip(sentence, labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mr. Johnson ,\n",
      "2 the CEO of XYZ Corp. ,\n",
      "3 arrived at the meeting at 9:00\n",
      "4 a.m. with his wife ,\n",
      "5 Mrs. Johnson !\n",
      "6 They were greeted by the board members ,\n",
      "7 including Dr. Anderson ,\n",
      "8 the company 's Chief Financial Officer ,\n",
      "9 and Ms. Thompson ,\n",
      "10 the Head of Marketing ?\n",
      "11 The meeting room was adorned with elegant decorations and state-of-the-art technology !\n",
      "12 During the meeting ,\n",
      "13 they discussed various topics ,\n",
      "14 including financial strategies ,\n",
      "15 marketing campaigns ,\n",
      "16 and employee engagement .\n",
      "17 They analyzed the current market trends ,\n",
      "18 reviewed sales reports ,\n",
      "19 and brainstormed innovative ideas .\n",
      "20 It was a high-energy session with everyone actively participating and sharing their perspectives !\n",
      "21 The meeting lasted for hours ,\n",
      "22 and they covered a wide range of issues .\n",
      "23 They deliberated on cost-cutting measures ,\n",
      "24 expansion plans ,\n",
      "25 and investment opportunities .\n",
      "26 They also addressed concerns raised by the staff ,\n",
      "27 such as work-life balance and career development !\n",
      "28 The atmosphere was intense yet collaborative ,\n",
      "29 with each person expressing their opinions respectfully .\n",
      "30 In addition to the main agenda ,\n",
      "31 they decided to implement new policies to improve customer satisfaction ,\n",
      "32 i.e.\n",
      "33 ,\n",
      "34 enhancing the quality of products and providing better after-sales service .\n",
      "35 They planned to conduct customer surveys ,\n",
      "36 gather feedback ,\n",
      "37 and tailor their offerings based on customer preferences .\n",
      "38 The focus was on delivering exceptional customer experiences and building long-term relationships !\n",
      "39 Furthermore ,\n",
      "40 they discussed their marketing strategy ,\n",
      "41 which included targeted advertising campaigns ,\n",
      "42 social media promotions ,\n",
      "43 and collaborations with influencers !\n",
      "44 They aimed to reach a wider audience and increase brand visibility .\n",
      "45 They also planned to launch a new product line targeting young consumers ,\n",
      "46 e.g.\n",
      "47 ,\n",
      "48 Gen Z\n",
      "49 and millennials .\n",
      "50 Extensive market research and product development were in progress to meet the specific needs and preferences of these demographics !\n",
      "51 In conclusion ,\n",
      "52 the meeting was productive ,\n",
      "53 and the decisions made will have a significant impact on the company 's future success !\n",
      "54 The collective efforts of the team ,\n",
      "55 led by Mr. Johnson and supported by the dedicated board members ,\n",
      "56 would propel the company to new heights !\n",
      "57 The commitment to excellence and constant innovation would position XYZ Corp. as a market leader and drive sustainable growth .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Custom sentence\n",
    "#custom_sentence = \"This is a custom sentence. It has multiple sentences. Can the model detect the boundaries?\"\n",
    "with open('sample.txt', 'r') as file:\n",
    "    custom_sentence = file.read()\n",
    "\n",
    "# Tokenize the custom sentence\n",
    "custom_tokens = nltk.word_tokenize(custom_sentence)\n",
    "\n",
    "# Extract features from the custom sentence\n",
    "custom_features = [extract_features(custom_tokens, i) for i in range(len(custom_tokens))]\n",
    "\n",
    "# Predict the labels for the custom sentence\n",
    "custom_labels = crf.predict([custom_features])[0]\n",
    "\n",
    "# Combine tokens based on the predicted labels\n",
    "separated_sentences = []\n",
    "current_sentence = []\n",
    "for token, label in zip(custom_tokens, custom_labels):\n",
    "    current_sentence.append(token)\n",
    "    if label == 'True':  # Sentence boundary detected\n",
    "        separated_sentences.append(\" \".join(current_sentence))\n",
    "        current_sentence = []\n",
    "\n",
    "# Add the last sentence if not already added\n",
    "if current_sentence:\n",
    "    separated_sentences.append(\" \".join(current_sentence))\n",
    "\n",
    "# Print the separated sentences\n",
    "i=0\n",
    "for sentence in separated_sentences:\n",
    "    i=i+1\n",
    "    print(i, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
